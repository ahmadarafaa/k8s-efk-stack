apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-environment-based-config
  namespace: logging
data:
  fluent.conf: |
    # Environment-Based Configuration
    # Routes logs based on environment prefixes (prod-, dev-, test-, staging-)
    
    # Source: Environment namespace logs
    <source>
      @type tail
      path /var/log/containers/*_production_*.log,/var/log/containers/*_development_*.log,/var/log/containers/*_logging-test_*.log,/var/log/containers/*_staging_*.log
      exclude_path ["/var/log/containers/fluentd*.log"]
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      <parse>
        @type regexp
        expression /^(?<time>.+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$/
        time_format %Y-%m-%dT%H:%M:%S.%N%:z
      </parse>
    </source>

    # Add Kubernetes metadata
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['KUBERNETES_URL']}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL']}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      skip_labels false
      skip_container_metadata false
      skip_master_url false
      skip_namespace_metadata false
    </filter>

    # Filter: Environment namespaces only
    <filter kubernetes.**>
      @type grep
      <regexp>
        key $['kubernetes']['namespace_name']
        pattern ^(production|development|logging-test|staging)$
      </regexp>
    </filter>

    # Extract environment and add routing metadata
    <filter kubernetes.**>
      @type record_transformer
      enable_ruby
      <record>
        environment ${record['kubernetes']['namespace_name'] == 'production' ? 'prod' : (record['kubernetes']['namespace_name'] == 'development' ? 'dev' : (record['kubernetes']['namespace_name'] == 'logging-test' ? 'test' : record['kubernetes']['namespace_name']))}
        service_name ${record['kubernetes']['namespace_name']}
        app_name ${record['kubernetes']['labels']['app'] || 'generic'}
        version ${record['kubernetes']['labels']['version'] || 'unknown'}
        release ${record['kubernetes']['labels']['release'] || 'unknown'}
        full_service_name ${record['kubernetes']['namespace_name']}-${record['kubernetes']['labels']['app'] || 'generic'}
        log_severity ${record['log'].match(/\b(TRACE|DEBUG|INFO|WARN|WARNING|ERROR|FATAL|CRITICAL)\b/i) ? record['log'].match(/\b(TRACE|DEBUG|INFO|WARN|WARNING|ERROR|FATAL|CRITICAL)\b/i)[0].upcase : 'INFO'}
        @timestamp ${Time.now.strftime('%Y-%m-%dT%H:%M:%S.%3NZ')}
      </record>
    </filter>

    # Parse structured application logs (JSON)
    <filter kubernetes.**>
      @type parser
      key_name log
      reserve_data true
      emit_invalid_record_to_error false
      <parse>
        @type json
      </parse>
    </filter>

    # Parse ALERT logs for compliance
    <filter kubernetes.**>
      @type parser
      key_name log
      reserve_data true
      emit_invalid_record_to_error false
      <parse>
        @type grok
        grok_pattern \[ALERT\]\s*user=(?<User>[^,]+),\s*action=(?<Action>[^,]+),\s*project=(?<Project>[^,]+),\s*status=(?<Status>[^,]+),\s*Field=(?<Field>[^,]+),\s*old_value=(?<Old_value>[^,]+),\s*new_value=(?<New_value>.+)
      </parse>
    </filter>

    # Route by environment and log type
    <match kubernetes.**>
      @type rewrite_tag_filter
      <rule>
        key log
        pattern /\[ALERT\]/
        tag alert.logs
      </rule>
      <rule>
        key log_severity
        pattern ERROR|FATAL|CRITICAL
        tag error.logs
      </rule>
      <rule>
        key environment
        pattern prod
        tag prod.logs
      </rule>
      <rule>
        key environment
        pattern dev
        tag dev.logs
      </rule>
      <rule>
        key environment
        pattern test
        tag test.logs
      </rule>
      <rule>
        key environment
        pattern staging
        tag staging.logs
      </rule>
    </match>

    # Output 1: ALERT logs (dynamic indexing by environment and service)
    <match alert.logs>
      @type elasticsearch
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"
      user "#{ENV['ELASTICSEARCH_USERNAME']}"
      password "#{ENV['ELASTICSEARCH_PASSWORD']}"
      scheme "#{ENV['ELASTICSEARCH_SCHEME']}"
      ssl_verify false
      
      index_name ${service_name}_${app_name}-alert-%Y-%m
      suppress_type_name true
      
      <buffer tag,service_name,app_name,time>
        @type file
        path /var/log/fluentd-buffers/alert
        timekey 86400  # Daily indices
        timekey_wait 300
        flush_mode interval
        flush_interval 30s
        retry_max_times 10
        chunk_limit_size 10MB
      </buffer>
    </match>

    # Output 2: Error logs (dynamic indexing by environment and service)
    <match error.logs>
      @type elasticsearch
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"
      user "#{ENV['ELASTICSEARCH_USERNAME']}"
      password "#{ENV['ELASTICSEARCH_PASSWORD']}"
      scheme "#{ENV['ELASTICSEARCH_SCHEME']}"
      ssl_verify false
      
      index_name ${service_name}_${app_name}-errors
      suppress_type_name true
      
      <buffer tag,service_name,app_name>
        @type file
        path /var/log/fluentd-buffers/errors
        flush_mode interval
        flush_interval 30s
        retry_max_times 10
        chunk_limit_size 5MB
      </buffer>
    </match>

    # Output 3: Production application logs (optimized performance)
    <match prod.logs>
      @type elasticsearch
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"
      user "#{ENV['ELASTICSEARCH_USERNAME']}"
      password "#{ENV['ELASTICSEARCH_PASSWORD']}"
      scheme "#{ENV['ELASTICSEARCH_SCHEME']}"
      ssl_verify false
      
      index_name ${service_name}_${app_name}
      suppress_type_name true
      
      <buffer tag,service_name,app_name>
        @type file
        path /var/log/fluentd-buffers/prod-apps
        timekey 86400  # Daily indices
        timekey_wait 300
        flush_mode interval
        flush_interval 10s
        retry_max_times 8
        chunk_limit_size 20MB
        compress gzip
      </buffer>
    </match>

    # Output 4: Development/Test logs (lower retention)
    <match dev.logs, test.logs>
      @type elasticsearch
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"
      user "#{ENV['ELASTICSEARCH_USERNAME']}"
      password "#{ENV['ELASTICSEARCH_PASSWORD']}"
      scheme "#{ENV['ELASTICSEARCH_SCHEME']}"
      ssl_verify false
      
      index_name ${service_name}_${app_name}
      suppress_type_name true
      
      <buffer tag,service_name,app_name>
        @type file
        path /var/log/fluentd-buffers/dev-test-apps
        flush_mode interval
        flush_interval 30s
        retry_max_times 3
        chunk_limit_size 10MB
      </buffer>
    </match>

    # Output 5: Staging logs (production-like)
    <match staging.logs>
      @type elasticsearch
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"
      user "#{ENV['ELASTICSEARCH_USERNAME']}"
      password "#{ENV['ELASTICSEARCH_PASSWORD']}"
      scheme "#{ENV['ELASTICSEARCH_SCHEME']}"
      ssl_verify false
      
      index_name ${service_name}_${app_name}
      suppress_type_name true
      
      <buffer tag,service_name,app_name>
        @type file
        path /var/log/fluentd-buffers/staging-apps
        flush_mode interval
        flush_interval 15s
        retry_max_times 5
        chunk_limit_size 15MB
      </buffer>
    </match>

    # Drop Fluentd internal logs
    <match fluent.**>
      @type null
    </match>

    # Output 6: Fallback for any unmatched logs
    <match **>
      @type elasticsearch
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"
      user "#{ENV['ELASTICSEARCH_USERNAME']}"
      password "#{ENV['ELASTICSEARCH_PASSWORD']}"
      scheme "#{ENV['ELASTICSEARCH_SCHEME']}"
      ssl_verify false
      
      index_name environment-generic
      suppress_type_name true
      
      <buffer>
        @type file
        path /var/log/fluentd-buffers/fallback
        flush_mode interval
        flush_interval 60s
        retry_max_times 2
        chunk_limit_size 5MB
      </buffer>
    </match>
